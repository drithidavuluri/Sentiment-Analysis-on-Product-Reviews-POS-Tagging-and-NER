# -*- coding: utf-8 -*-
"""NLU Assignment 1 Q1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/nlu-assignment-1-q1-eeb76351-8130-4d6b-91c8-f3863c2020dd.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240220/auto/storage/goog4_request%26X-Goog-Date%3D20240220T182128Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0f40fcf5a61d67451fcbd9d263535c2827b9a2e0c488d17f1b0aad301d77ac25a39873366359a8be45c100068e14e286ef7d883127e71b6ab472bcf1a7dce32a6e5a2a2dbcd6404c78fb311fa69f2440f632bcd4f9018de3a08cbcedd1a04ccaff575cc3cf9b5e8386aeae1d4c5a3a72e8d32bd136217083670e2065cdd11b6275d2657891f6befc24d787ec79c9cbc15e5c9c72027f5592b5554c601a2c0c8d6470019d73ee145dac58d91e6f87a14c4d87fcd697cba0bbcf4a708006db4057c586050fd376cc1c91471763e8143bb166073100fea588355a0f784bd428bb66c55534c0d77ca29d9181898b447bd4c27dbde2b9b72d2523796f035524aaa0ed
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'ner-dataset-nlu:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4462392%2F7654203%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240220%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240220T182128Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6436da9464106fd65a2866a37340cff05ec017486b96b09f0d63ad8b1714ed4df82c2889c18011f2d39167f5be117f55b948f19de2b5f82e52608f6fbf33658701223dae265d51b50a62e9c658f56e66bd71ae818c98e96f946f0f275dd7928efaa30071df06cdb33591d8c48f997e602c3b3f59ddb5d24b5901a542a91fbaabe2c4128f5488b1b3809e37a7e9222a9aee884ca47f83f4dce23ce1e5cb4fdd459b6bf11c250eef7ceebbd6ae20c37d6a5a1dcc4b8c3149abd545cca50ccf335f054ce5045b47279fc5389a99cefed46fe4734797e9133e073110f65ebef8acedc3a7856e4ac002f9889f83bb87d9a9e966409884a81d1b98c130aa9f76130aa7,processed-nlu1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4468774%2F7663387%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240220%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240220T182128Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D673f557a7ca28ad4da7963cfc9bab2e547a1cfd8c05cf20efdffe6ed9095ba2e6eb87f9bf25c45444f01e9fa3a6a647f7fbd0810dd7c8db1a5f986c36def012199b9b6d584bfbdb3c6a3e36f6b11cf3df493231ee2b1a14e9d3dc7146c5e3e49b9d97c98ca9de674a97be5aeb0b823e6a6ab9908208623c54faa5eafbb52f0730c8996bce8a3b1acabf08e1e0998b4d3541d80d0492c51725a4e3a7d59485928cc456e7f029b21eca399b0a8a29edf473a72444b712e2336f72fe3d0188c410bc3a7745da7bd4a457bc902eb90ba194f6e6da679ee9310d77814d95932b59fa100e2653c3c6ebc3363f79c68650feadb94c5d8754170a55d36d96f34c498b569'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""# Task 1"""

import pandas as pd
import numpy as np
import string
import warnings
warnings.filterwarnings("ignore")
import ast
import re
!pip install pytorch
!pip install spacy
import spacy
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import StanfordNERTagger
from sklearn.metrics import accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import precision_score, recall_score, f1_score

"""# Loading Dataset"""

df= pd.read_csv('/kaggle/input/ner-dataset-nlu/NER_Dataset.csv')

df.head()

len(df)

df['sentence']=df['Word']

df

"""# Data Preprocessing"""

df = df.drop(columns=['Sentence_ID'])

word_list=df['Word'][0]

word_list

word_list_string = ast.literal_eval(word_list)
sentence_form = ' '.join(word_list_string)

sentence_form

df

for i in range(len(df)):
    word_list = df['sentence'][i]
    word_list_string = ast.literal_eval(word_list)
    sentence = ' '.join(word_list_string)
    df.at[i, 'sentence'] = sentence

df

spacy_tag_mapping = {
    "B-art": "ART",
    "B-eve": "EVENT",
    "B-geo": "GPE",
    "B-gpe": "GPE",
    "B-nat": "NORP",
    "B-org": "ORG",
    "B-per": "PERSON",
    "B-tim": "TIME",
    "I-art": "ART",
    "I-eve": "EVENT",
    "I-geo": "GPE",
    "I-gpe": "GPE",
    "I-nat": "NORP",
    "I-org": "ORG",
    "I-per": "PERSON",
    "I-tim": "TIME",
    "O": ""
}

nltk_tag_mapping = {
    "B-art": "ART",
    "B-eve": "EVENT",
    "B-geo": "GPE",
    "B-gpe": "GPE",
    "B-nat": "NORP",
    "B-org": "ORG",
    "B-per": "PERSON",
    "B-tim": "TIME",
    "I-art": "ART",
    "I-eve": "EVENT",
    "I-geo": "GPE",
    "I-gpe": "GPE",
    "I-nat": "NORP",
    "I-org": "ORG",
    "I-per": "PERSON",
    "I-tim": "TIME",
    "O": ""
}

stanford_ner_tag_mapping = {
    "B-art": "ART",
    "B-eve": "EVENT",
    "B-geo": "LOCATION",
    "B-gpe": "LOCATION",
    "B-nat": "LOCATION",
    "B-org": "ORGANIZATION",
    "B-per": "PERSON",
    "B-tim": "DATE",
    "I-art": "ART",
    "I-eve": "EVENT",
    "I-geo": "LOCATION",
    "I-gpe": "LOCATION",
    "I-nat": "LOCATION",
    "I-org": "ORGANIZATION",
    "I-per": "PERSON",
    "I-tim": "DATE",
    "O": ""
}

df['spacy_tag']=df['Tag']
df['nltk_tag']=df['Tag']
df['stanford_ner']=df['Tag']

for i in range(len(df)):
    word_list = df['Tag'][i]
    df['Tag'][i] = ast.literal_eval(word_list)

df

def map_tags(word_list, dictionary_map):
    transformed_list = []
    for word in word_list:
        if word in dictionary_map:
            transformed_list.append(dictionary_map[word])
        else:
            transformed_list.append(word)
    return transformed_list

for i in range(len(df['spacy_tag'])):
    word_list_string = ast.literal_eval(df['spacy_tag'][i])
    df['spacy_tag'][i]=map_tags(word_list_string, spacy_tag_mapping)

for i in range(len(df['nltk_tag'])):
    word_list_string = ast.literal_eval(df['nltk_tag'][i])
    df['nltk_tag'][i]=map_tags(word_list_string, spacy_tag_mapping)

for i in range(len(df['spacy_tag'])):
    word_list_string = ast.literal_eval(df['stanford_ner'][i])
    df['stanford_ner'][i]=map_tags(word_list_string, spacy_tag_mapping)

df

# for i in range(l):
#     review= re.sub('[^a-zA-Z]', ' ', df['sentence'][i])
#     review=review.lower()
#     df['sentence'][i]=review

df['sentence'][0]

"""# Part 1

# a)
"""

nlp = spacy.load("en_core_web_sm")

df['pos_spacy']=df['sentence']
df['ner_spacy']=df['sentence']

for i in range(len(df)):
    doc = nlp(df['sentence'][i])
    df['pos_spacy'][i] = [(token.text, token.tag_) for token in doc]
    df['ner_spacy'][i] = [(ent.text, ent.label_) for ent in doc.ents]

df

"""## b)"""

def pos_tag_nltk(sentence):
    tokens = word_tokenize(sentence)
    pos_tags = nltk.pos_tag(tokens)
    return pos_tags

def ner_nltk_func(sentence):
    tokens = word_tokenize(sentence)
    ner_tags = nltk.ne_chunk(nltk.pos_tag(tokens))
    return ner_tags

df['pos_nltk']=df['sentence']
df['ner_nltk']=df['sentence']

for i in range(len(df)):
    df['pos_nltk'][i] = pos_tag_nltk(df['sentence'][i])
    df['ner_nltk'][i] = ner_nltk_func(df['sentence'][i])

df['POS'][0]

df['pos_nltk'][0]

print(df['ner_nltk'][0])

df

"""## c)"""

stanford_ner_model = 'path_to_stanford_ner_model'
stanford_ner_jar = 'path_to_stanford_ner_jar'
st = StanfordNERTagger(stanford_ner_model, stanford_ner_jar, encoding='utf-8')

def pos_tag_stanford_ner(sentence):
    tokens = word_tokenize(sentence)
    pos_tags = st.tag(tokens)
    return pos_tags

def ner_stanford_ner(sentence):
    tokens = word_tokenize(sentence)
    ner_tags = st.tag(tokens)
    return ner_tags

df['pos_stanford']=df['sentence']
df['ner_stanford']=df['sentence']

for i in range(len(df)):
    df['pos_stanford'][i] = pos_tag_stanford_ner(df['sentence'][i])
    df['ner_stanford'][i] = ner_stanford_ner(df['sentence'][i])

"""# Part 3

## NLTK
"""

for i in range(len(df)):
    word_list = df['pos_nltk'][i]
    df['pos_nltk'][i] = ast.literal_eval(word_list)

df

for i in range(len(df)):
    list=[]
    for j in range(len(df['pos_nltk'][i])):
        list.append(df['pos_nltk'][i][j][1])
    df['pos_nltk'][i]=list

for i in range(len(df)):
    list=[]
    for j in range(len(df['pos_spacy'][i])):
        list.append(df['pos_spacy'][i][j][1])
    df['pos_spacy'][i]=list

df

# for i in range(len(df)):
#     word_list = df['ner_nltk'][i]
#     df['ner_nltk'][i] = ast.literal_eval(word_list)

type(df['POS'][0])

for i in range(len(df)):
    word_list = df['POS'][i]
    df['POS'][i] = ast.literal_eval(word_list)

df

df['POS']

df['pos_nltk']

# Filter out rows with unequal lengths of POS lists
filtered_df = df[df['POS'].apply(len) == df['pos_nltk'].apply(len)]

# Extract ground truth and predicted data
nltk_ground_truth = filtered_df['POS'].values
nltk_predicted_data = filtered_df['pos_nltk'].values

# Flatten the nested lists
nltk_ground_truth_flat = [item for sublist in nltk_ground_truth for item in sublist]
nltk_predicted_data_flat = [item for sublist in nltk_predicted_data for item in sublist]

# Compute accuracy
nltk_accuracy = accuracy_score(nltk_ground_truth_flat, nltk_predicted_data_flat)

nltk_accuracy

nltk_unique_tags = np.unique(nltk_ground_truth_flat + nltk_predicted_data_flat)
nltk_tags_list = nltk_unique_tags.tolist()

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='', cmap='Blues', xticklabels=nltk_tags_list, yticklabels=nltk_tags_list)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""## SpaCy"""

# Filter out rows with unequal lengths of POS lists
spacy_filtered_df = df[df['POS'].apply(len) == df['pos_spacy'].apply(len)]

# Extract ground truth and predicted data
spacy_ground_truth = spacy_filtered_df['POS'].values
spacy_predicted_data = spacy_filtered_df['pos_spacy'].values

# Flatten the nested lists
spacy_ground_truth_flat = [item for sublist in spacy_ground_truth for item in sublist]
spacy_predicted_data_flat = [item for sublist in spacy_predicted_data for item in sublist]

# Compute accuracy
spacy_accuracy = accuracy_score(spacy_ground_truth_flat, spacy_predicted_data_flat)

spacy_accuracy

spacy_unique_tags = np.unique(spacy_ground_truth_flat + spacy_predicted_data_flat)
spacy_tags_list = spacy_unique_tags.tolist()

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='', cmap='Blues', xticklabels=spacy_tags_list, yticklabels=spacy_tags_list)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""* The NLTK tagger achieves slightly higher accuracy than spaCy on this dataset - 96.25% vs 95.80%. This indicates NLTK may be doing a better job overall at predicting the correct POS tags.

* However, looking at the confusion matrices, NLTK seems to struggle more with certain tag pairs like NN/NNS (noun singular/plural) and VBP/VBZ (verb present tense singular/plural) based on the higher values off the diagonal.

# Part 4

* The NLTK tagger achieves slightly higher accuracy than spaCy on this dataset - 96.25% vs 95.80%. This indicates NLTK may be doing a better job overall at predicting the correct POS tags.

* However, looking at the confusion matrices, NLTK seems to struggle more with certain tag pairs like NN/NNS (noun singular/plural) and VBP/VBZ (verb present tense singular/plural) based on the higher values off the diagonal.

### More evaluation metrices
"""

nltk_precision = precision_score(nltk_ground_truth_flat, nltk_predicted_data_flat, average='weighted')
nltk_recall = recall_score(nltk_ground_truth_flat, nltk_predicted_data_flat, average='weighted')
nltk_f1_score = f1_score(nltk_ground_truth_flat, nltk_predicted_data_flat, average='weighted')

spacy_precision = precision_score(spacy_ground_truth_flat, spacy_predicted_data_flat, average='weighted')
spacy_recall = recall_score(spacy_ground_truth_flat, spacy_predicted_data_flat, average='weighted')
spacy_f1_score = f1_score(spacy_ground_truth_flat, spacy_predicted_data_flat, average='weighted')

print(nltk_precision)
print(nltk_recall)
print(nltk_f1_score)

print(spacy_precision)
print(spacy_recall)
print(spacy_f1_score)

"""Based on the evaluation metrics obtained:

**NLTK Model:**
- Precision: 96.32%
- Recall: 96.25%
- F1-score: 96.26%

**SpaCy Model:**
- Precision: 96.27%
- Recall: 95.80%
- F1-score: 95.89%

Comparative Analysis:

1. **Precision:**
   - NLTK model has a slightly higher precision than SpaCy, indicating that it tends to make fewer false positive predictions.

2. **Recall:**
   - NLTK model also has a slightly higher recall than SpaCy, suggesting that it captures more of the actual positives.

3. **F1-score:**
   - NLTK model's F1-score is marginally higher than SpaCy, implying that it achieves a slightly better balance between precision and recall.

Overall, both models perform well, with NLTK showing a slightly better performance across all metrics. However, the differences are relatively small, indicating that both models are comparable in terms of their effectiveness for POS tagging tasks. Other factors such as ease of use, computational efficiency, and available features may also influence the choice between these two models for specific applications.

# Part 5

The confusion matrices reveal that spaCy better distinguishes between similar noun and verb tags indicating it leverages contextual information well. For example, spaCy has lower confusion between singular and plural nouns (NN/NNS) and present tense verbs (VBP/VBZ) likely because it utilizes surrounding semantics.

However, NLTK demonstrates strengths in identifying certain POS tags like comparative adjectives (JJ/JJR) and prepositions (IN) where its hand-crafted rules offer control. This enables NLTK to correctly tag some irregular cases that depend less on context

SpaCy's neural network provides robustness in using context for disambiguation but lacks the flexibility of NLTK's feature engineering. An ideal system would incorporate both - spaCy's contextual modeling and NLTK's rules-based approach. While spaCy is convenient out-of-the-box, NLTK allows customization that could improve domain-specific accuracy.

# Part 6

We have implemented the mapping of tags already wherever it needs to be done in the code. We used the following statergy-
1. Tag Mapping Strategy
2. Consistency in Tagging
3. Enhanced Model Performance
4. Facilitating Interoperability
5. Scalability and Adaptability
"""